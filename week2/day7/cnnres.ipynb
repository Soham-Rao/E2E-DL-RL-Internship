{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa93b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models, callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753d9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1,28,28,1).astype('float32') / 255.0\n",
    "x_test  = x_test.reshape(-1,28,28,1).astype('float32') / 255.0\n",
    "\n",
    "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test_oh  = tf.keras.utils.to_categorical(y_test,  num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e750aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, dropout_rate=0.3):\n",
    "    shortcut = x\n",
    "    reg = tf.keras.regularizers.l2(0.001)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1, padding='same', kernel_regularizer=reg)(shortcut)\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    reg = tf.keras.regularizers.l2(0.001)\n",
    "\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=reg)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587beb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_cnn()\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * 0.95**epoch)\n",
    "\n",
    "# model.fit(\n",
    "#     x_train, y_train_oh,\n",
    "#     validation_split=0.1,\n",
    "#     epochs=50,\n",
    "#     batch_size=128,\n",
    "#     callbacks=[lr_schedule, early_stop],\n",
    "#     verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14982224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "106/106 [==============================] - 7s 42ms/step - loss: 1.4353 - accuracy: 0.7525 - val_loss: 3.1648 - val_accuracy: 0.1050 - lr: 0.0100\n",
      "Epoch 2/40\n",
      "106/106 [==============================] - 4s 38ms/step - loss: 0.8796 - accuracy: 0.9560 - val_loss: 3.0018 - val_accuracy: 0.1050 - lr: 0.0100\n",
      "Epoch 3/40\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.7948 - accuracy: 0.9713 - val_loss: 2.5460 - val_accuracy: 0.1583 - lr: 0.0100\n",
      "Epoch 4/40\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.7400 - accuracy: 0.9783 - val_loss: 2.0334 - val_accuracy: 0.3212 - lr: 0.0100\n",
      "Epoch 5/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.6971 - accuracy: 0.9823 - val_loss: 1.4420 - val_accuracy: 0.7600 - lr: 0.0100\n",
      "Epoch 6/40\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.6603 - accuracy: 0.9851 - val_loss: 1.2548 - val_accuracy: 0.8623 - lr: 0.0100\n",
      "Epoch 7/40\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.6295 - accuracy: 0.9865 - val_loss: 0.9474 - val_accuracy: 0.9665 - lr: 0.0100\n",
      "Epoch 8/40\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.6012 - accuracy: 0.9884 - val_loss: 0.9433 - val_accuracy: 0.9648 - lr: 0.0100\n",
      "Epoch 9/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.5771 - accuracy: 0.9896 - val_loss: 0.8780 - val_accuracy: 0.9690 - lr: 0.0100\n",
      "Epoch 10/40\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.5545 - accuracy: 0.9907 - val_loss: 0.8121 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 11/40\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.5341 - accuracy: 0.9911 - val_loss: 0.7967 - val_accuracy: 0.9525 - lr: 0.0100\n",
      "Epoch 12/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.5158 - accuracy: 0.9919 - val_loss: 0.6459 - val_accuracy: 0.9847 - lr: 0.0100\n",
      "Epoch 13/40\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.5000 - accuracy: 0.9926 - val_loss: 0.6193 - val_accuracy: 0.9892 - lr: 0.0100\n",
      "Epoch 14/40\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4855 - accuracy: 0.9929 - val_loss: 0.5988 - val_accuracy: 0.9897 - lr: 0.0100\n",
      "Epoch 15/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.4729 - accuracy: 0.9936 - val_loss: 0.6129 - val_accuracy: 0.9820 - lr: 0.0100\n",
      "Epoch 16/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.4595 - accuracy: 0.9937 - val_loss: 0.5787 - val_accuracy: 0.9887 - lr: 0.0100\n",
      "Epoch 17/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.4489 - accuracy: 0.9940 - val_loss: 0.5738 - val_accuracy: 0.9838 - lr: 0.0100\n",
      "Epoch 18/40\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.4395 - accuracy: 0.9942 - val_loss: 0.5875 - val_accuracy: 0.9830 - lr: 0.0100\n",
      "Epoch 19/40\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4306 - accuracy: 0.9949 - val_loss: 0.5363 - val_accuracy: 0.9885 - lr: 0.0100\n",
      "Epoch 20/40\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.4222 - accuracy: 0.9949 - val_loss: 0.5373 - val_accuracy: 0.9860 - lr: 0.0100\n",
      "Epoch 21/40\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4152 - accuracy: 0.9951 - val_loss: 0.4646 - val_accuracy: 0.9905 - lr: 0.0100\n",
      "Epoch 22/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.4077 - accuracy: 0.9955 - val_loss: 0.5077 - val_accuracy: 0.9858 - lr: 0.0100\n",
      "Epoch 23/40\n",
      "105/106 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.9951\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4008 - accuracy: 0.9951 - val_loss: 0.4830 - val_accuracy: 0.9892 - lr: 0.0100\n",
      "Epoch 24/40\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.3944 - accuracy: 0.9966 - val_loss: 0.4585 - val_accuracy: 0.9922 - lr: 0.0050\n",
      "Epoch 25/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3886 - accuracy: 0.9971 - val_loss: 0.4664 - val_accuracy: 0.9892 - lr: 0.0050\n",
      "Epoch 26/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.3847 - accuracy: 0.9974 - val_loss: 0.4518 - val_accuracy: 0.9917 - lr: 0.0050\n",
      "Epoch 27/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3831 - accuracy: 0.9974 - val_loss: 0.4697 - val_accuracy: 0.9890 - lr: 0.0050\n",
      "Epoch 28/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.3794 - accuracy: 0.9976 - val_loss: 0.4502 - val_accuracy: 0.9912 - lr: 0.0050\n",
      "Epoch 29/40\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.3766 - accuracy: 0.9978 - val_loss: 0.4299 - val_accuracy: 0.9925 - lr: 0.0050\n",
      "Epoch 30/40\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.3749 - accuracy: 0.9976 - val_loss: 0.4424 - val_accuracy: 0.9885 - lr: 0.0050\n",
      "Epoch 31/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.3727 - accuracy: 0.9974 - val_loss: 0.4130 - val_accuracy: 0.9933 - lr: 0.0050\n",
      "Epoch 32/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.3699 - accuracy: 0.9980 - val_loss: 0.4437 - val_accuracy: 0.9865 - lr: 0.0050\n",
      "Epoch 33/40\n",
      "105/106 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.9980\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3676 - accuracy: 0.9980 - val_loss: 0.4220 - val_accuracy: 0.9923 - lr: 0.0050\n",
      "Epoch 34/40\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.3650 - accuracy: 0.9983 - val_loss: 0.4026 - val_accuracy: 0.9927 - lr: 0.0025\n",
      "Epoch 35/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3625 - accuracy: 0.9988 - val_loss: 0.4073 - val_accuracy: 0.9932 - lr: 0.0025\n",
      "Epoch 36/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.3610 - accuracy: 0.9989 - val_loss: 0.4004 - val_accuracy: 0.9933 - lr: 0.0025\n",
      "Epoch 37/40\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.3599 - accuracy: 0.9991 - val_loss: 0.4050 - val_accuracy: 0.9932 - lr: 0.0025\n",
      "Epoch 38/40\n",
      "105/106 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.9992\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3588 - accuracy: 0.9992 - val_loss: 0.4066 - val_accuracy: 0.9925 - lr: 0.0025\n",
      "Epoch 39/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3573 - accuracy: 0.9994 - val_loss: 0.4009 - val_accuracy: 0.9935 - lr: 0.0012\n",
      "Epoch 40/40\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 0.3565 - accuracy: 0.9993 - val_loss: 0.3918 - val_accuracy: 0.9943 - lr: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16648e44fa0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.95, nesterov=True)\n",
    "lr_schedule1 = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "lr_schedule2 = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.01 * 0.95**epoch, verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05)\n",
    "regularizer = tf.keras.regularizers.L2(0.001)\n",
    "\n",
    "model = build_cnn()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn , metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_oh, epochs=40, batch_size=512, validation_split=0.1, callbacks=[lr_schedule1, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3aa340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3910 - accuracy: 0.9954 - 1s/epoch - 5ms/step\n",
      "\n",
      "✅ Test Accuracy: 99.5400\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test_oh, verbose=2)\n",
    "print(f\"\\n✅ Test Accuracy: {test_acc*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa555d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
